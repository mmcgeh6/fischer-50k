---
phase: 02-data-retrieval-waterfall
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - lib/storage.py
autonomous: true

must_haves:
  truths:
    - "Building_Metrics table exists in Supabase with BBL as primary key"
    - "Table has all 11 bare minimum fields as typed columns"
    - "Table has all use-type square footage fields as NUMERIC columns"
    - "Upsert function inserts new buildings and updates existing ones"
    - "Timestamps (created_at, updated_at) are automatically managed"
  artifacts:
    - path: "lib/storage.py"
      provides: "Building_Metrics table creation, upsert operations, timestamp trigger"
      exports: ["create_building_metrics_table", "upsert_building_metrics"]
  key_links:
    - from: "lib/storage.py"
      to: "Supabase PostgreSQL"
      via: "psycopg2 direct connection"
      pattern: "psycopg2\\.connect|ON CONFLICT.*DO UPDATE"
---

<objective>
Create the Building_Metrics storage table and upsert module for persisting aggregated building data.

Purpose: Phase 2 needs a destination table for waterfall results. The Building_Metrics table is the central store for all aggregated building data (identity, energy, mechanical references). This plan creates the schema and the Python module to write to it.

Output: `lib/storage.py` with table creation DDL, upsert function, and timestamp trigger. Table created in Supabase.
</objective>

<execution_context>
@C:\Users\minke\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\minke\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-data-retrieval-waterfall/02-RESEARCH.md
@.planning/codebase/INTEGRATIONS.md
@lib/database.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create lib/storage.py with Building_Metrics schema and upsert logic</name>
  <files>lib/storage.py</files>
  <action>
Create `lib/storage.py` with these components:

**1. Database connection function** using psycopg2 direct connection (NOT st.connection — this module needs to work outside Streamlit context for future batch processing). Load credentials from environment variables with fallback to dotenv. Connection details from CLAUDE.md:
- Host: aws-0-us-west-2.pooler.supabase.com
- Port: 5432
- Database: postgres
- User: postgres.lhtuvtfqjovfuwuxckcw
- Password: loaded from .env or Streamlit secrets (never hardcoded in this file)
- SSL: require

**2. `create_building_metrics_table()` function** that executes CREATE TABLE IF NOT EXISTS with:

Identity fields (from LL97/GeoSearch - Step 1):
- `bbl VARCHAR(10) PRIMARY KEY`
- `bin VARCHAR(10)`
- `address TEXT`
- `zip_code VARCHAR(5)`
- `compliance_pathway VARCHAR(200)`

Building characteristics (from LL84/PLUTO - Step 2):
- `year_built INTEGER`
- `property_type VARCHAR(200)`
- `gfa NUMERIC` (gross floor area)
- `energy_star_score INTEGER`

Energy metrics (from LL84 - Step 2, the 5 energy bare minimum fields):
- `electricity_kwh NUMERIC`
- `natural_gas_kbtu NUMERIC`
- `fuel_oil_kbtu NUMERIC`
- `steam_kbtu NUMERIC`
- `site_eui NUMERIC`

All use-type square footage fields as NUMERIC columns. Use snake_case naming derived from the use type name. The full list includes 60 columns total (not 42). Explanation: CLAUDE.md references "42 Primary Use Types from LL84" but the full Building_Metrics schema must also include the 15 emissions-factor-only use types that appear in LL97 penalty calculations (Bowling Alley, Convenience Store without Gas Station, Library, Lifestyle Center, Personal Services, Vocational School, and 9 "Other" categories) plus 3 additional LL84 sub-types (Hotel Gym, Barracks, Convention Center). These additional types may receive sqft values from manual input or future data sources, and are needed for penalty calculations in Phase 3. Convert each to snake_case + `_sqft` suffix:
- adult_education_sqft, ambulatory_surgical_center_sqft, automobile_dealership_sqft, bank_branch_sqft, college_university_sqft, convenience_store_with_gas_station_sqft, courthouse_sqft, data_center_sqft, distribution_center_sqft, enclosed_mall_sqft, financial_office_sqft, fire_station_sqft, fitness_center_health_club_gym_sqft, food_sales_sqft, food_service_sqft, hospital_general_medical_surgical_sqft, hotel_sqft, k_12_school_sqft, laboratory_sqft, library_sqft, lifestyle_center_sqft, mailing_center_post_office_sqft, medical_office_sqft, mixed_use_property_sqft, movie_theater_sqft, multifamily_housing_sqft, museum_sqft, non_refrigerated_warehouse_sqft, office_sqft, other_sqft, other_education_sqft, other_entertainment_public_assembly_sqft, other_lodging_residential_sqft, other_mall_sqft, other_public_services_sqft, other_recreation_sqft, other_services_sqft, other_technology_science_sqft, other_utility_sqft, outpatient_rehabilitation_physical_therapy_sqft, parking_sqft, performing_arts_sqft, police_station_sqft, pre_school_daycare_sqft, prison_incarceration_sqft, refrigerated_warehouse_sqft, residence_hall_dormitory_sqft, residential_care_facility_sqft, restaurant_sqft, retail_store_sqft, self_storage_facility_sqft, senior_care_community_sqft, social_meeting_hall_sqft, strip_mall_sqft, supermarket_grocery_store_sqft, swimming_pool_sqft, urgent_care_clinic_other_outpatient_sqft, wastewater_treatment_plant_sqft, wholesale_club_supercenter_sqft, worship_facility_sqft

LL87 reference fields (Step 3):
- `ll87_audit_id INTEGER`
- `ll87_period VARCHAR(20)`

Data source tracking:
- `data_source VARCHAR(100)` (comma-separated sources used, e.g. 'll97,ll84_api,ll87')
- `created_at TIMESTAMPTZ DEFAULT NOW()`
- `updated_at TIMESTAMPTZ DEFAULT NOW()`

Also create indexes:
- `idx_building_metrics_bin ON building_metrics(bin)`
- `idx_building_metrics_updated ON building_metrics(updated_at)`

Also create the `update_updated_at_column()` trigger function and attach it to building_metrics table (BEFORE UPDATE trigger). Use `CREATE OR REPLACE FUNCTION` and `DROP TRIGGER IF EXISTS` before `CREATE TRIGGER` for idempotency.

**3. `upsert_building_metrics(building_data: Dict[str, Any])` function** that:
- Accepts a dict with any subset of the Building_Metrics columns (keys matching column names)
- Builds a dynamic INSERT...ON CONFLICT (bbl) DO UPDATE statement from the provided keys
- Only updates columns that are present in the input dict (not all columns)
- Returns the bbl, created_at, updated_at of the upserted row
- Uses parameterized queries (no string interpolation for values)

**4. `get_building_metrics(bbl: str)` function** that:
- Returns the full row from building_metrics for a given BBL as a dict
- Returns None if BBL not found

**5. Export a `USE_TYPE_SQFT_COLUMNS` list** containing all use-type column names, so other modules can reference the exact column names.

Important: Do NOT use Streamlit's st.connection for this module. Use psycopg2 directly with connection parameters from environment variables (DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD) or from .env file via python-dotenv. This ensures the module works in both Streamlit and batch processing contexts.

For the .env loading: Check if Streamlit secrets are available first (import streamlit and check st.secrets), fall back to os.environ, then fall back to dotenv. Wrap Streamlit import in try/except so the module works without Streamlit installed.
  </action>
  <verify>
Run: `python -c "from lib.storage import create_building_metrics_table, upsert_building_metrics, get_building_metrics, USE_TYPE_SQFT_COLUMNS; print(f'Columns: {len(USE_TYPE_SQFT_COLUMNS)}'); print('Import OK')"` — should print column count (60) and "Import OK".

Then run: `python -c "from lib.storage import create_building_metrics_table; create_building_metrics_table(); print('Table created')"` — should create table in Supabase without errors.

Then run: `python -c "from lib.storage import upsert_building_metrics, get_building_metrics; upsert_building_metrics({'bbl': '0000000000', 'address': 'TEST ROW'}); row = get_building_metrics('0000000000'); print(f'Test row: {row[\"address\"]}'); "` — should print "Test row: TEST ROW".

Clean up test row after verification.
  </verify>
  <done>
  - lib/storage.py exists with create_building_metrics_table(), upsert_building_metrics(), get_building_metrics(), and USE_TYPE_SQFT_COLUMNS
  - Building_Metrics table created in Supabase with BBL primary key, 11 base fields, 60 use-type sqft columns (42 LL84 primary + 18 emissions-factor/sub-type columns for penalty calc coverage), timestamps
  - updated_at trigger function installed and attached
  - Upsert correctly inserts new rows and updates existing rows on BBL conflict
  - Module works with psycopg2 directly (no Streamlit dependency for core operations)
  </done>
</task>

</tasks>

<verification>
- `python -c "from lib.storage import create_building_metrics_table, upsert_building_metrics, get_building_metrics, USE_TYPE_SQFT_COLUMNS; print('All exports available')"` succeeds
- building_metrics table exists in Supabase (verify with `SELECT count(*) FROM building_metrics`)
- Upsert test: insert a row, update it, verify updated_at changed
- USE_TYPE_SQFT_COLUMNS list has 60 entries
</verification>

<success_criteria>
- Building_Metrics table exists in Supabase with correct schema (BBL PK, 11 base fields, 60 use-type sqft columns, timestamps)
- Upsert function works correctly (insert new, update existing, dynamic column handling)
- Timestamp trigger auto-updates updated_at on row changes
- Module importable and functional without Streamlit
</success_criteria>

<output>
After completion, create `.planning/phases/02-data-retrieval-waterfall/02-01-SUMMARY.md`
</output>
