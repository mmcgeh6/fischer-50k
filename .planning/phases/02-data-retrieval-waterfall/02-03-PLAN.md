---
phase: 02-data-retrieval-waterfall
plan: 03
type: execute
wave: 2
depends_on: ["02-01", "02-02"]
files_modified:
  - lib/waterfall.py
  - lib/database.py
  - app.py
autonomous: false

must_haves:
  truths:
    - "User enters BBL and system executes full 3-step waterfall (LL97 identity, LL84 live energy, LL87 mechanical)"
    - "System falls back to PLUTO-then-GeoSearch chain if BBL not in LL97 table"
    - "System falls back to PLUTO if LL84 API returns no data"
    - "System saves all retrieved data to Building_Metrics table after waterfall"
    - "UI shows data from live API fetch, not just pre-loaded database tables"
    - "System tracks data sources used for each building"
  artifacts:
    - path: "lib/waterfall.py"
      provides: "3-step waterfall orchestrator"
      exports: ["fetch_building_waterfall"]
    - path: "lib/database.py"
      provides: "Updated to query Building_Metrics table for cached results"
    - path: "app.py"
      provides: "Updated UI wired to waterfall pipeline"
  key_links:
    - from: "lib/waterfall.py"
      to: "lib/nyc_apis.py"
      via: "imports call_geosearch_api, call_ll84_api, call_pluto_api"
      pattern: "from lib\\.nyc_apis import"
    - from: "lib/waterfall.py"
      to: "lib/storage.py"
      via: "imports upsert_building_metrics"
      pattern: "from lib\\.storage import"
    - from: "lib/waterfall.py"
      to: "lib/database.py"
      via: "imports fetch_building_by_bbl for LL97/LL87 queries"
      pattern: "from lib\\.database import"
    - from: "app.py"
      to: "lib/waterfall.py"
      via: "imports fetch_building_waterfall"
      pattern: "from lib\\.waterfall import"
---

<objective>
Create the waterfall orchestrator that ties together identity resolution, live API fetching, and storage, then wire it into the Streamlit UI.

Purpose: This is the central plan that brings Phase 2 together. The waterfall orchestrator executes Steps 1-3 of the 5-step process (Identity, LL84 Live Fetch, LL87 Mechanical), aggregates results, saves to Building_Metrics, and returns the combined data. The UI is updated to use this waterfall instead of only querying pre-loaded database tables.

Output: `lib/waterfall.py` (orchestrator), updated `lib/database.py` (add Building_Metrics query), updated `app.py` (wire to waterfall).
</objective>

<execution_context>
@C:\Users\minke\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\minke\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-data-retrieval-waterfall/02-RESEARCH.md
@.planning/phases/02-data-retrieval-waterfall/02-01-SUMMARY.md
@.planning/phases/02-data-retrieval-waterfall/02-02-SUMMARY.md
@lib/database.py
@lib/nyc_apis.py
@lib/storage.py
@lib/validators.py
@app.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create lib/waterfall.py orchestrator and update lib/database.py</name>
  <files>lib/waterfall.py, lib/database.py</files>
  <action>
**Create `lib/waterfall.py`** with the main waterfall orchestrator:

```python
def fetch_building_waterfall(bbl: str, save_to_db: bool = True) -> Dict[str, Any]:
```

This function executes the 3-step data retrieval waterfall:

**Step 1: Identity & Compliance** (DATA-01, DATA-02)
- Query LL97 Covered Buildings table by BBL (use existing database.py function logic, but extracted as a standalone query -- see below)
- If found in LL97: Extract BBL, BIN, canonical address, zip_code, compliance pathway. Track data source as `'ll97'`.
- If NOT found in LL97: Execute the **PLUTO-to-GeoSearch fallback chain** as follows:
  1. Call `call_pluto_api(bbl)` to query PLUTO by BBL. PLUTO returns building data including the `address` field (street address) mapped in Plan 02-02.
  2. If PLUTO returns an address: Call `call_geosearch_api(pluto_result['address'])` with that address to resolve the BIN. GeoSearch requires an address as input (not a BBL), which is why PLUTO must be called first.
  3. If GeoSearch succeeds: Use the BIN from GeoSearch. Also store the PLUTO address as the canonical address (since we have no LL97 canonical address). Track data source as `'pluto,geosearch'`.
  4. If PLUTO returns no address OR GeoSearch fails: Log a warning and continue with whatever data we have. The waterfall should NOT crash -- if we have no BIN, Step 2 (LL84) will simply be skipped since LL84 requires a BIN. Track data source accordingly.
  5. If PLUTO itself returns None (BBL not in PLUTO): Log error, continue with BBL only. No BIN available, so LL84 will be skipped.

**Step 2: Live Usage Fetch** (DATA-03, DATA-05, DATA-06, DATA-07)
- Only execute if we have a BIN from Step 1 (either from LL97 or from the GeoSearch fallback)
- Call `call_ll84_api(bin_number)` using BIN from Step 1
- If LL84 returns data: merge energy metrics + all use-type sqft fields into result dict. Track source: append `',ll84_api'`
- If LL84 returns None: Call `call_pluto_api(bbl)` as fallback (DATA-05) if not already called in Step 1. PLUTO provides year_built, gfa, but NOT energy metrics (those will be None). Track source: append `',pluto'`
- If no BIN available (Step 1 fallback chain failed to resolve one): Skip LL84 entirely, try PLUTO for basic building metrics if not already called. Log warning "No BIN available, skipping LL84 API fetch".

**Step 3: Mechanical Retrieval** (DATA-04)
- Query LL87 raw table by BBL (reuse existing logic from database.py)
- Search 2019-2024 first, fallback to 2012-2018 (DISTINCT ON with CASE ordering, same as existing query)
- If found: Store ll87_audit_id, ll87_period, and raw_data in result dict
- Track data source: append `',ll87'`

**After all 3 steps:**
- Set `data_source` field to the comma-separated list of sources used
- If `save_to_db=True`: Call `upsert_building_metrics(result)` to persist to Building_Metrics table. Only pass the fields that Building_Metrics knows about (exclude ll87_raw JSONB -- that stays in the ll87_raw table). Include all 11 base fields + use-type sqft fields + ll87_audit_id + ll87_period + data_source.
- Return the full result dict (including ll87_raw for UI display and narrative generation)

The function should use Python logging (`logger = logging.getLogger(__name__)`):
- INFO: "Step 1: Resolving identity for BBL {bbl}"
- INFO: "Step 1: BBL found in LL97 table"
- WARNING: "Step 1: BBL not in LL97, executing PLUTO->GeoSearch fallback chain"
- INFO: "Step 1: PLUTO returned address '{address}', querying GeoSearch for BIN"
- INFO: "Step 1: GeoSearch resolved BIN {bin} from PLUTO address"
- WARNING: "Step 1: GeoSearch failed to resolve BIN from PLUTO address"
- WARNING: "Step 1: PLUTO returned no address for BBL, cannot resolve BIN"
- INFO: "Step 2: Fetching LL84 data for BIN {bin}"
- WARNING: "Step 2: No BIN available, skipping LL84 API fetch"
- WARNING: "Step 2: LL84 data not found, falling back to PLUTO"
- INFO: "Step 3: Retrieving LL87 mechanical data for BBL {bbl}"
- INFO: "Waterfall complete for BBL {bbl}, sources: {data_source}"

For LL97 and LL87 queries: Create helper functions in this module that use psycopg2 directly (import the connection function from storage.py) rather than st.connection. This ensures the waterfall works outside Streamlit. The queries are the same as in database.py but using psycopg2 instead of st.connection:
- `_query_ll97(bbl: str) -> Optional[Dict]` -- same SQL as database.py ll97_query
- `_query_ll87(bbl: str) -> Optional[Dict]` -- same SQL as database.py ll87_query

**Update `lib/database.py`** to add a function that queries the Building_Metrics table:

```python
def fetch_building_from_metrics(bbl: str) -> Optional[Dict[str, Any]]:
```
- Query building_metrics table by BBL using st.connection (for Streamlit caching)
- Return the full row as a dict, or None if not found
- This provides a "check cache first" option for the UI

Also add:
```python
def check_building_processed(bbl: str) -> Optional[str]:
```
- Check if BBL exists in building_metrics table
- Return the updated_at timestamp as string if exists, None if not
- Used by UI to show "last processed" status
  </action>
  <verify>
Test the waterfall with a known BBL from the LL97 table: `python -c "from lib.waterfall import fetch_building_waterfall; result = fetch_building_waterfall('1001580001', save_to_db=True); print(f'Address: {result.get(\"address\")}, Sources: {result.get(\"data_source\")}, Energy: {result.get(\"electricity_kwh\")}')"` — should print address, data sources used, and energy value (or None if LL84 has no data for this building).

Verify data was saved: `python -c "from lib.storage import get_building_metrics; row = get_building_metrics('1001580001'); print(f'Saved: {row[\"address\"] if row else \"Not found\"}')"` — should show the saved address.

Test database.py additions: `python -c "from lib.database import check_building_processed; ts = check_building_processed('1001580001'); print(f'Processed: {ts}')"` — should show timestamp.
  </verify>
  <done>
  - lib/waterfall.py exists with fetch_building_waterfall() executing all 3 steps
  - Step 1 queries LL97 table, falls back to PLUTO->GeoSearch chain (PLUTO by BBL gets address, GeoSearch with address gets BIN)
  - Step 2 calls LL84 API (requires BIN), falls back to PLUTO if no LL84 data
  - Step 3 queries LL87 raw table with dual-dataset protocol
  - Results saved to Building_Metrics via upsert
  - Data source tracking records which APIs were used
  - lib/database.py updated with fetch_building_from_metrics() and check_building_processed()
  </done>
</task>

<task type="auto">
  <name>Task 2: Wire waterfall into Streamlit UI</name>
  <files>app.py</files>
  <action>
Update `app.py` to use the waterfall pipeline instead of only querying pre-loaded tables:

**Changes to the form submission handler (around line 195-223):**

Replace the current flow:
```python
building_data = fetch_building_by_bbl(bbl_input)
```

With a two-phase approach:
1. First check if building already exists in Building_Metrics (cache check):
   ```python
   from lib.database import check_building_processed
   cached_ts = check_building_processed(bbl_input)
   ```
2. If cached, offer a choice to use cached data or re-fetch:
   - Show "Last processed: {timestamp}" info
   - Add a checkbox: "Re-fetch live data" (default: unchecked if cached within last 24h)
3. Execute waterfall:
   ```python
   from lib.waterfall import fetch_building_waterfall
   building_data = fetch_building_waterfall(bbl_input, save_to_db=True)
   ```
4. Update the spinner text to reflect the multi-step process: "Running data retrieval waterfall (LL97 -> LL84 API -> LL87)..."

**Changes to data display:**
- After waterfall completes, show a small info box indicating which data sources were used: `st.info(f"Data sources: {building_data.get('data_source', 'unknown')}")`
- If LL84 API returned no data (PLUTO fallback was used), show a warning: `st.warning("LL84 energy data not available for this building. Using PLUTO fallback for basic building metrics.")`
- If LL87 data was not found, note it in the Energy Data tab

**Changes to imports:**
- Add: `from lib.waterfall import fetch_building_waterfall`
- Add: `from lib.database import check_building_processed`
- Keep existing imports (they're still used for display functions)

**Session state updates:**
- Store the data source string as `st.session_state['data_source']` so it persists across Streamlit reruns
- Store the last processed timestamp as `st.session_state['last_processed']` so the cache-check UI can display it without re-querying

**Important:** Keep the existing display functions (display_building_info, display_energy_data, display_penalties, display_narratives) unchanged — they work with the same dict format. The waterfall returns a compatible dict.

Do NOT change the narrative generation flow — that stays the same (generate_all_narratives is called after waterfall returns). Narratives use the ll87_raw data from the waterfall result, which is in the same format as before.

Ensure the app still works if Building_Metrics table doesn't exist yet (graceful degradation — fall back to old fetch_building_by_bbl behavior with a warning).
  </action>
  <verify>
Run `streamlit run app.py` and test with a known BBL (e.g., 1001580001):
1. Enter BBL and submit — should show spinner with "Running data retrieval waterfall..."
2. After completion — should show data source info (e.g., "Data sources: ll97,ll84_api,ll87")
3. Building Info tab should show BBL, BIN, address, year built, etc.
4. Energy Data tab should show electricity, gas, fuel oil, steam values (from live LL84 API)
5. Second lookup of same BBL should show "Last processed" timestamp
  </verify>
  <done>
  - app.py imports and calls fetch_building_waterfall instead of only fetch_building_by_bbl
  - UI shows data source indicators (which APIs contributed data)
  - UI shows PLUTO fallback warnings when LL84 data missing
  - UI shows "last processed" timestamp for previously processed buildings
  - Session state keys `st.session_state['data_source']` and `st.session_state['last_processed']` used for cross-rerun persistence
  - Existing display functions work unchanged with waterfall output
  - Narrative generation still works with ll87_raw from waterfall
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete 3-step data retrieval waterfall with live NYC API integration, Building_Metrics storage, and updated Streamlit UI</what-built>
  <how-to-verify>
1. Run `streamlit run app.py` in the project directory
2. Enter a known BBL (try `1001580001` or any BBL from the LL97 list)
3. Verify the spinner shows multi-step waterfall progress
4. Check the "Data sources" info shows which APIs were queried
5. Verify the Building Info tab shows identity data (BBL, BIN, address)
6. Verify the Energy Data tab shows live energy metrics (electricity, gas, etc.) — values should come from the LL84 API, not the pre-loaded ll84_data table
7. Verify that looking up the same BBL again shows "Last processed" timestamp
8. Try a BBL that might NOT be in the LL97 list to test fallback behavior
9. Check that the System Narratives tab still generates narratives from LL87 data
  </how-to-verify>
  <resume-signal>Type "approved" if the waterfall works end-to-end, or describe any issues you see</resume-signal>
</task>

</tasks>

<verification>
- Waterfall executes all 3 steps in sequence for a given BBL
- Data persists in Building_Metrics table after waterfall
- UI reflects live API data (not just pre-loaded static data)
- Fallback logic works: LL97 miss -> PLUTO (gets address) -> GeoSearch (gets BIN) -> LL84 (uses BIN)
- PLUTO fallback also works when LL84 returns no data (Step 2 fallback)
- Data source tracking shows which APIs contributed
- Existing narrative generation still works
- `python -c "from lib.waterfall import fetch_building_waterfall; from lib.storage import get_building_metrics; fetch_building_waterfall('1001580001'); row = get_building_metrics('1001580001'); print('PASS' if row else 'FAIL')"` prints PASS
</verification>

<success_criteria>
- User enters BBL, system executes full 3-step waterfall with live API calls
- System correctly falls back (PLUTO->GeoSearch chain for identity when not in LL97, PLUTO for energy data when LL84 missing)
- All retrieved data saved to Building_Metrics table with upsert logic
- UI shows live data with source indicators
- Building_Metrics tracks processing timestamps
- Narrative generation works with waterfall output
</success_criteria>

<output>
After completion, create `.planning/phases/02-data-retrieval-waterfall/02-03-SUMMARY.md`
</output>
